# WO-4 — Forward meet closure

**Goal:** build the order-free (monotone, extensive, idempotent) **forward meet** (F[p,k,c]) and apply the **color-agnostic lift**; emit Π-safe test masks (A_{p,c}) for the test grid. Deterministic, byte-exact, CPU-fast.

## Read these anchors before coding

* ` @docs/anchors/01_addendum.md ` §2 (Mask algebra closure), §14 (No hidden search)
* ` @docs/anchors/02_addendum.md ` §H (closure operator statement)
* ` @docs/anchors/03_annex.md ` A.1–A.3 (byte-exact equality, int64 only, lex rules)
* ` @docs/anchors/04_engg_spec.md ` §6 (Forward meet & mask)

**Math reminder (why order-free):** a closure operator is **extensive, monotone, idempotent**; this guarantees order-independent fixpoints (least closed element) — exactly what we want for “shuffle trainings, same mask.” ([Wikipedia][1])

---

## Mature libraries & exact functions (no algorithm invention)

* **NumPy (only)**

  * Boolean/equality & broadcasting: `np.array_equal`, `==`, `np.logical_and` ([NumPy][2])
  * Index extraction: `np.where(condition)` (one-arg form gives indices / “nonzero”) ([NumPy][3])
  * Stable sorting for harness shuffles (if you need canonicalization): `np.sort(..., kind='stable')` (NumPy reference) ([NumPy][4])

No SciPy needed here. No RNG. No threads/JIT. All comparisons are **byte-exact** (no tolerances) per anchors.

---

## Module: `src/arcsolver/mask.py`

### 1) API — forward meet & mask

```python
def build_forward_meet(
    train_pairs_emb_aligned: list[tuple[np.ndarray, np.ndarray]],  # (X_i_emb, Y_i_emb)
    H_out: int,
    W_out: int,
    C_in: int = 10,   # palette size for X
    C_out: int = 10,  # palette size for Y (post-alignment)
) -> np.ndarray:
    """
    Returns F of shape (H_out*W_out, C_in, C_out) with dtype=bool, where
    F[p,k,c] == True  iff every training with X_i[p]==k also has Y_i[p]==c,
    after closure (monotone, extensive, idempotent) and color-agnostic lift.
    """
```

#### Implementation (vectorized, order-free)

* Let (N = H_\text{out} W_\text{out}); flatten all grids in **C-order** once.
* Initialize `F = np.ones((N, C_in, C_out), dtype=bool)` (extensive start).

**Meet step (intersection across trainings)**
For each training ((X_i, Y_i)):

* For each input color (k), build boolean index `idx_k = (X_i_flat == k)`.
* For those positions, **keep only** the actually observed output color `c* = Y_i_flat[idx_k]` by setting `F[idx_k, k, :] = False` then `F[idx_k, k, c*] = True`.
  This simultaneously performs:

  * “forward meet” (F[p,k,*] \gets F[p,k,*] \land \mathbf{1}_{c=c^*}), and
  * the **row-local sufficiency** (“if (X_i(p)=k) and (Y_i(p)=c^*), then forbid (c\ne c^*)”).
    Use boolean assignment; no loops over pixels. Use pure NumPy broadcasting & boolean masks (logical ops are defined element-wise) ([NumPy][5]) and one-arg `np.where` if you need positions ([NumPy][3]).

> Note: because every time we touch (F[p,k,:]) we replace it with the singleton observed for that training, the global result is the **intersection** across trainings and hence **order-free** (commutative) — a standard closure property. ([Wikipedia][1])

**Fixpoint (idempotence)**

* After processing **all** trainings once, you are already at the meet of all constraints; running the same pass again must not change `F` (idempotence). We will **assert** that in receipts.

**Color-agnostic lift**

* For each pixel (p), collect the set (K_\text{obs}(p)={k: \exists i, X_i(p)=k}).
* If (K_\text{obs}(p)\neq\emptyset) and the **admit vectors** `F[p,k,:]` are **identical** for all (k\in K_\text{obs}), then for all unseen (k'\notin K_\text{obs}(p)) set `F[p,k',:] = that_identical_vector`.
  Implement with:

  * Build a boolean `seen[p,k]` by OR-reducing `(X_i_flat == k)` over trainings.
  * For each `p`, gather rows `F[p,seen[p,:],:]` and test `np.array_equal` across them (byte-exact) ([NumPy][2]); if equal, copy to unseen rows.
    This exactly matches the addendum’s **color-agnostic lift**.

**Test mask for the test grid**

```python
def build_test_mask(F: np.ndarray, Xstar_emb: np.ndarray) -> np.ndarray:
    """
    Returns A_{p,c} bool of shape (N, C_out) with A[p,c] = F[p, Xstar[p], c].
    """
```

### 2) Optional utilities for receipts/progress

```python
def admits_stats(F: np.ndarray) -> dict:
    """
    Returns statistics: avg_admits_per_pixel (mean over p of sum_c F[p,*,c] for seen ks),
    total_true, and a histogram of admits-set sizes, to show closure tightening.
    """
```

---

## Receipts (first-class)

For each task `<tid>`, write `receipts/<tid>/wo04.json`:

```json
{
  "stage": "wo04",
  "mask": {
    "F_shape": [N, 10, 10],
    "F_hash": "<sha256>",                   // hash over F.view(uint8).tobytes(order="C")
    "idempotent_ok": true,                  // recompute once: identical hash
    "closure_order_independent_ok": true,   // same F_hash after 3 shuffled training orders
    "avg_admits_per_pixel": 1.37,
    "total_true": 123456
  },
  "lift": {
    "pixels_with_unified_admits": 789,      // where lift applied
    "copied_rows": 4567
  },
  "A_mask": {
    "A_shape": [N, 10],
    "A_hash": "<sha256>"
  }
}
```

* Hash with SHA-256 over raw bytes (C-order) to fingerprint `F` / `A` deterministically. ([NumPy][6])
* `closure_order_independent_ok`: recompute `F` after shuffling training order (e.g., 3 random **permutations**, but with a **fixed seed** or deterministic permutations) and compare hashes — must match, because closure is order-free (monotone + idempotent). ([Wikipedia][1])

---

## Harness changes (progress & metrics)

**`src/arcsolver/harness.py`**
Add support for `--upto-wo 4`:

1. Load each **dict-based** ARC JSON (glob `*.json` → open → iterate `train`/`test` keys).
2. Ensure trainings are **already embedded & color-aligned** (from WO-2/3). If running WO-4 stand-alone, use embedded & aligned outputs that harness produces in earlier steps.
3. Build `F` with `build_forward_meet(...)`, compute `A` with `build_test_mask(...)`.
4. Compute two hashes, the **idempotence** hash (rebuild once), and the **order-independence** hashes by repeating with shuffled order (use **deterministic permutations**, e.g., reverse order and a fixed cyclic shift; no RNG).
5. Write `wo04.json` receipts and record **progress metrics**:

```python
acc_bool(progress, "closure_order_independent_ok", closure_order_independent_ok)
acc_sum(progress,  "avg_admits_before", avg_before)  # optional if you also log a “before”
acc_sum(progress,  "avg_admits_after",  avg_after)   # from admits_stats(F)
```

*(The generic accumulator from your harness patch already supports these; you just call it.)*

---

## Reviewer instructions (exact)

Run on all 1000 tasks:

```bash
bash scripts/run_harness.sh --upto-wo 4 --strict
```

Verify:

* Each task has `receipts/<tid>/wo04.json`; re-run once — receipts must be **byte-identical**.
* Open `progress/progress_wo04.json` and report:

  * `closure_order_independent_ok = 100%`
  * The aggregate `avg_admits_after` ≤ `avg_admits_before` (non-increasing), showing closure **tightening**.
* Spot check a few tasks:

  * `idempotent_ok: true` (rebuild hash equals).
  * `A_hash` stable across two runs.

**Any red metric is an implementation gap in WO-4** (never “dataset UNSAT”). Fix and re-run.

---

## Anti-optimization guard (CPU)

* All passes are O(m·N·C) boolean ops; ARC sizes are tiny. No threading/JIT. No “clever” sparse tricks. Correctness first.

---

## Adapters & “hard improvements” enforced here

* **Order-free closure**: guaranteed by (extensive, monotone, idempotent) design; we assert with **shuffle checks**. ([Wikipedia][1])
* **Idempotence**: re-run once; hashes must match (Π²=Π).
* **Byte-exact** ops only (`==`, `array_equal`, `logical_and`). ([NumPy][2])
* **Deterministic hashing** for receipts (C-order bytes). ([NumPy][6])

---

## Deliverables checklist (implementer)

* [ ] `build_forward_meet(...)` produces `F: (N,10,10) bool` using vectorized boolean assignment; no loops over pixels.
* [ ] `build_test_mask(...)` returns `A: (N,10) bool` via `F[p, Xstar[p], :]` (vectorized gather).
* [ ] `admits_stats(F)` for receipt metrics.
* [ ] Harness `--upto-wo 4` path: writes `wo04.json`; records `closure_order_independent_ok`, `avg_admits_*`.
* [ ] No floats; all dtypes correct; byte-exact equality only.

## Acceptance checklist (reviewer)

* [ ] `closure_order_independent_ok = 100%`.
* [ ] `idempotent_ok = true` for spot checks; receipts byte-stable across runs.
* [ ] `avg_admits_after ≤ avg_admits_before` (aggregate); per-task stats look sane.
* [ ] Any failure is an **implementation gap** in WO-4; fix before WO-5.

---

### Notes on the math (for the reviewer, FYI)

* The “shuffle ⇒ same mask” test is exactly the closure operator law (monotone + idempotent ⇒ order-independent meet/fixpoint). ([Wikipedia][1])
* You don’t need labels to verify this; it’s a structural (Π/GLUE) receipt.
